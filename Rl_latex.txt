\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Reinforcement Learning}
\author{Tirthankar Mittra }
\date{April 2021}

\begin{document}
\maketitle
\section{Formulas}
Expected reward for state-action pairs.
$$r(s,a)=E[R_{t+1}|S_{t}=s,A_{t}=a]=\sum_{r\epsilon R}{r}\sum_{s^{'}\epsilon S}{p(s^{'},r|s,a)}$$
\newline
Expected reward for state-action-next state triplets...
\begin{align}
P(a,b|c)&=\frac{P(a \bigcap b \bigcap c)}{P(c)} \\ \nonumber
&=\frac{P(a \bigcap b \bigcap c)}{P(b \bigcap c)}*\frac{P(b \bigcap c)}{P(c)} \\ \nonumber
&=P(a|b,c)*P(b|c)
\end{align}
\begin{align} \nonumber
r(s,a,s^{'})&=E[R_{t+1}|S_{t}=s,A_{t}=a,S_{t+1}=s^{'}] \\ \nonumber
&=\sum_{r\epsilon R}{r}p(r|s,a,s^{'}) \\ \nonumber
&=\sum_{r\epsilon R}{r}\frac{p(s^{'},r|s,a)}{p(s^{'}|s,a)} ... from (1) \\ \nonumber
&=\frac{\sum_{r\epsilon R}{r p(s^{'},r|s,a)}}{\sum_{r\epsilon R}{p(s^{'},r|s,a)}}
\end{align}
\newline

The value of state $s$ under a policy $\pi$, denoted by $v_{\pi}(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter. 
$$
v_{\pi}(s)=E_{\pi}[G_{t}|S_{t}=s]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s]
$$
\newline
Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted by $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
We call $q_{\pi}$ the action-value function for policy $\pi$.
\begin{align}\nonumber
q_{\pi}(s,a)&=E_{\pi}[G_{t}|S_{t}=s,A_{t}=a]\\ \nonumber &=E_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s,A_{t}=a]\nonumber
\end{align}
Some expectations formulas
\begin{align}
&E[A|B]=\sum_{A}{A*P(A|B)}\\
&E[A+B|C]=\sum_{i=0}^{N}{(A_{i}+B_{i})*P(A_{i}+B_{i}|C)}\\
\end{align}
A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. 
\end{document}
